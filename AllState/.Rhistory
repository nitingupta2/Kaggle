?caret::createDataPartition
lFolds <- caret::createDataPartition(dfTrain$loss, times = 10)
summary(lFolds)
summary(lFolds[[1]])
summary(lFolds[[2]])
summary(lFolds[[3]])
lapply(lFolds, summary)
head(lFolds[[1]])
head(lFolds[[2]])
str(lFolds[[2]])
idx_test <- lFolds[[i]]
dfTest_fold <- dfTrain[idx_test,]
dfTrain_fold <- dfTrain[!idx_test,]
idx_test <- lFolds[[i]]
dfTest_fold <- dfTrain[idx_test,]
dfTrain_fold <- dfTrain[-idx_test,]
lapply(lFolds, summary)
lapply(lFolds, length)
str(idx_test)
str(dfTrain$id)
dfTest_fold <- dfTrain %>% dplyr::filter(id == idx_test)
dfTest_fold <- dfTrain %>% dplyr::filter(id %in% idx_test)
head(idx_test)
head(dfTrain$id)
dfTest_fold <- dfTrain[idx_test,]
dfTrain_fold <- dfTrain[-(idx_test),]
lFolds <- caret::createDataPartition(dfTrain$loss, times = numFolds, list = F)
head(lFolds)
lFolds <- caret::createDataPartition(dfTrain$loss, times = numFolds)
i <- 1
dfTest_fold <- dfTrain[idx_test,]
dfTrain_fold <- dfTrain[-(idx_test),]
lFolds <- caret::createDataPartition(dfTrain$loss, times = numFolds, groups = 10)
lFolds <- caret::createDataPartition(dfTrain$loss, times = numFolds, groups = 10, list = F)
head(lFolds)
head(dfTrain)
head(lFolds)
lFolds <- caret::createDataPartition(dfTrain$loss, times = numFolds)
idx_test <- lFolds[[i]]
dfTest_fold <- dfTrain[idx_test,]
dfTrain_fold <- dfTrain[-idx_test,]
head(idx_test)
length(idx_test)
tail(idx_test)
x <- c(1,2,3)
idx_test <- c(1,2,3)
dfTest_fold <- dfTrain[idx_test,]
dfTrain_fold <- dfTrain[-idx_test,]
idx_test <- lFolds[[i]]
str(idx_test)
dfTest_fold <- dfTrain[idx_test,]
dfTrain_fold <- dfTrain[-idx_test,]
numFolds
lFolds <- caret::createDataPartition(dfTrain$loss, times = numFolds, p = (numFolds-1)/numFolds)
idx_train <- lFolds[[i]]
dfTrain_fold <- dfTrain[idx_train,]
dfTest_fold <- dfTrain[-idx_train,]
numFolds
lFolds <- caret::createDataPartition(dfTrain$loss, times = numFolds, p = (numFolds-1)/numFolds)
lapply(lFolds, summary)
source('C:/Backups/Kaggle/AllState/model_ensemble.R', echo=TRUE)
source('C:/Backups/Kaggle/AllState/model_ensemble.R', echo=TRUE)
head(dfPred_train)
head(dfPred_test)
numFolds <- 5
lFolds <- caret::createDataPartition(dfTrain$loss, times = numFolds, p = (numFolds-1)/numFolds)
idx_train <- lFolds[[i]]
dfTrain_fold <- dfTrain[idx_train,]
dfTest_fold <- dfTrain[-idx_train,]
source('C:/Backups/Kaggle/AllState/model_ensemble.R', echo=TRUE)
lapply(lFolds, head)
lFolds <- caret::createFolds(dfTrain$loss, k = numFolds)
lapply(lFolds, head)
lapply(lFolds, summary)
lapply(lFolds, head)
lapply(lFolds, summary)
lapply(lFolds, head)
idx_train <- lFolds[[i]]
idx_train <- lFolds[[i]]
dfTrain_fold <- dfTrain[idx_train,]
dfTest_fold <- dfTrain[-idx_train,]
idx_test <- lFolds[[i]]
dfTest_fold <- dfTrain[idx_test,]
dfTrain_fold <- dfTrain[-idx_test,]
source('C:/Backups/Kaggle/AllState/model_ensemble.R', echo=TRUE)
library(magrittr)
dfPred_train <- do.call(cbind.data.frame, lPred_train)
dfPred_train <- dfPred_train %>%
set_colnames(vModels) %>%
rownames_to_column(var = "id")
write.csv(dfPred_train, file = "ensemble_train.csv", row.names = F, quote = F)
dfPred_test <- as.data.frame(lPred_test)
dfPred_test <- dfPred_test %>%
set_colnames(vModels) %>%
rownames_to_column(var = "id")
write.csv(dfPred_test, file = "ensemble_test.csv", row.names = F, quote = F)
head(dfPred_train)
head(dfPred_test)
head(vTest)
head(lResults)
head(lResults[[1]])
head(lResults$pred_test)
head(lResults$pred_oof)
tail(lResults$pred_test)
head(dfPred_test)
tail(dfPred_test)
names(dfTest)
head(dfTest)
head(dfTest$loss)
head(lPred_test[[2]])
head(lPred_test[[1]])
head(dfPred_train)
dfPred_train$loss <- dfTrain$loss
head(dfPred_train)
ggplot(dfPred_train, aes(x = loss, y = xgboost)) + geom_point()
ggplot(dfPred_train, aes(x = loss, y = mxnet)) + geom_point()
ggplot(dfPred_train, aes(x = loss, y = xgboost)) + geom_point() + xlim(c(0,50000))
source('C:/Backups/Kaggle/AllState/model_ensemble.R', echo=TRUE)
source('C:/Backups/Kaggle/AllState/model_ensemble.R', echo=TRUE)
head(dfPred_test)
head(dfPred_train)
dfPred_train$loss <- dfTrain$loss
ggplot(dfPred_train, aes(x = loss, y = xgboost)) + geom_point() + xlim(c(0,30000))
head(dfPred_train)
ggplot(dfPred_train, aes(x = loss, y = xgboost)) + geom_point()
head(dfPred_test)
tail(dfPred_test)
source('C:/Backups/Kaggle/AllState/model_ensemble.R', echo=TRUE)
head(dfPred_test)
head(dfPred_train)
dfPred_train <- dfPred_train %>% mutate(xgboost = xgboost/5, loss = dfTrain$loss)
head(dfPred_train)
numFolds
source('C:/Backups/Kaggle/AllState/model_ensemble.R', echo=TRUE)
dfPred_train <- dfPred_train %>% mutate(loss = dfTrain$loss)
head(dfPred_train)
ggplot(dfPred_train, aes(x = loss, y = xgboost)) + geom_point()
ggplot(dfPred_train, aes(x = loss, y = xgboost)) + geom_point() + xlim(c(0,30000)) + ylim(c(0,30000))
dfPred_test
dfPred_train <- read.csv(file = "ensemble_train_mxnet.csv")
dfPred_train <- dfPred_train %>% mutate(loss = dfTrain$loss)
ggplot(dfPred_train, aes(x = loss, y = mxnet)) + geom_point() + xlim(c(0,30000)) + ylim(c(0,30000))
dfPred_test <- read.csv(file = "ensemble_test_mxnet.csv")
head(dfPred_test)
head(dfPred_train)
ggplot(dfPred_train, aes(x = loss, y = mxnet)) + geom_point()
dfPred_train <- do.call(cbind.data.frame, lPred_train)
dfPred_train <- dfPred_train %>%
set_colnames(vModels) %>%
rownames_to_column(var = "id")
dfPred_train <- dfPred_train %>% mutate(loss = dfTrain$loss)
head(dfPred_train)
ggplot(dfPred_train, aes(x = loss, y = xgboost)) + geom_point()
dfPred_test <- as.data.frame(lPred_test)
dfPred_test <- dfPred_test %>%
set_colnames(vModels) %>%
rownames_to_column(var = "id")
head(dfPred_test)
seedForFolds <- 1802
set.seed(seedForFolds)
lFolds <- caret::createFolds(dfTrain$loss, k = numFolds)
print(lapply(lFolds, summary))
print(lapply(lFolds, head, 15))
seedForFolds <- 4414
set.seed(seedForFolds)
lFolds <- caret::createFolds(dfTrain$loss, k = numFolds)
print(lapply(lFolds, summary))
print(lapply(lFolds, head, 15))
source('C:/Backups/Kaggle/AllState/model_ensemble.R', echo=TRUE)
dfPred_train <- dfPred_train %>% mutate(loss = dfTrain$loss)
head(dfPred_train)
head(dfPred_test)
ggplot(dfPred_train, aes(x = loss, y = xgboost)) + geom_point()
ggplot(dfPred_train, aes(x = loss, y = xgboost)) + geom_point() + xlim(c(0,30000)) + ylim(c(0,30000))
Metrics::mae(dfPred_train$loss, dfPred_train$xgboost)
source('C:/Backups/Kaggle/AllState/model_ensemble.R', echo=TRUE)
head(dfPred_train)
head(dfPred_test)
dfPred_train <- dfPred_train %>% mutate(loss = dfTrain$loss)
ggplot(dfPred_train, aes(x = loss, y = xgboost)) + geom_point() + xlim(c(0,30000)) + ylim(c(0,30000))
df <- dfPred_test %>% select(id, xgboost) %>% rename(loss = xgboost)
head(df)
write.csv(df, file = "submission_xgboost_avg.csv", quote = F, row.names = F)
Metrics::mae(dfPred_train$loss, dfPred_train$xgboost)
Metrics::mae(dfPred_train$loss, dfPred_train$mxnet)
ggplot(dfPred_train, aes(x = mxnet, y = xgboost)) + geom_point() + xlim(c(0,30000)) + ylim(c(0,30000))
ggplot(dfPred_train, aes(x = loss, y = mxnet)) + geom_point() + xlim(c(0,30000)) + ylim(c(0,30000))
ggplot(dfPred_train, aes(x = loss, y = mxnet)) + geom_point() + scale_x_log10() + scale_y_log10()
options(scipen = 5, width = 120)
ggplot(dfPred_train, aes(x = loss, y = xgboost)) + geom_point() + scale_x_log10() + scale_y_log10()
ggplot(dfPred_train, aes(x = loss, y = mxnet)) + geom_point() + scale_x_log10() + scale_y_log10()
dfPred_train <- do.call(cbind.data.frame, lPred_train)
dfPred_train <- dfPred_train %>%
set_colnames(paste("loss", vModels, sep = "_")) %>%
rownames_to_column(var = "id")
head(dfPred_train)
dfPred_train <- do.call(cbind.data.frame, lPred_train)
dfPred_train <- dfPred_train %>%
set_colnames(paste("loss", vModels, sep = "_")) %>%
rownames_to_column(var = "id")
write.csv(dfPred_train, file = "ensemble_train.csv", row.names = F, quote = F)
dfPred_test <- as.data.frame(lPred_test)
dfPred_test <- dfPred_test %>%
set_colnames(paste("loss", vModels, sep = "_")) %>%
rownames_to_column(var = "id")
write.csv(dfPred_test, file = "ensemble_test.csv", row.names = F, quote = F)
head(dfPred_test)
print(paste("xgboost CV:", Metrics::mae(dfTrain$loss, dfPred_train$loss_xgboost)))
print(paste("mxnet CV:", Metrics::mae(dfTrain$loss, dfPred_train$loss_mxnet)))
head(dfPred_train)
head(dfPred_test)
?xgb.train
?xgb.cv
y_train <- dfTrain$loss
x_train <- xgb.DMatrix(as.matrix(dfPred_train %>% select(-id)), label=y_train)
x_test <- xgb.DMatrix(as.matrix(dfPred_test %>% select(-id)))
lParams_ensemble_xgb <- list(eta = 0.01,
max_depth = 4,
min_child_weight = 1,
subsample = 0.8,
colsample_bytree = 0.5,
objective = "reg:linear",
eval_metric = "mae",
nthreads = 4)
set.seed(SEED)
cv_res <- xgb.cv(lParams_ensemble_xgb,
x_train,
nrounds=15000,
nfold=4,
early_stopping_rounds=50,
print_every_n = 50,
verbose= 1,
maximize=FALSE)
best_nrounds <- cv_res$best_iteration
cv_mean <- cv_res$evaluation_log$test_mae_mean[best_nrounds]
cv_stdev <- cv_res$evaluation_log$test_mae_std[best_nrounds]
print(paste("CV_Mean:", cv_mean))
print(paste("CV_Stdev:", cv_stdev))
y_train <- dfTrain$loss
x_train <- xgb.DMatrix(as.matrix(dfPred_train %>% select(-id)), label=y_train)
x_test <- xgb.DMatrix(as.matrix(dfPred_test %>% select(-id)))
lParams_ensemble_xgb <- list(eta = 0.01,
max_depth = 4,
min_child_weight = 1,
subsample = 0.8,
colsample_bytree = 0.5,
objective = "reg:linear",
eval_metric = "mae",
nthreads = 4)
set.seed(SEED)
cv_res <- xgb.cv(lParams_ensemble_xgb,
x_train,
nrounds=15000,
nfold=4,
early_stopping_rounds=150,
print_every_n = 50,
verbose= 1,
maximize=FALSE)
best_nrounds <- cv_res$best_iteration
cv_mean <- cv_res$evaluation_log$test_mae_mean[best_nrounds]
cv_stdev <- cv_res$evaluation_log$test_mae_std[best_nrounds]
print(paste("CV_Mean:", cv_mean))
print(paste("CV_Stdev:", cv_stdev))
y_train <- dfTrain$loss
x_train <- xgb.DMatrix(as.matrix(dfPred_train %>% select(-id)), label=y_train)
x_test <- xgb.DMatrix(as.matrix(dfPred_test %>% select(-id)))
lParams_ensemble_xgb <- list(eta = 0.001,
max_depth = 4,
min_child_weight = 1,
subsample = 0.8,
colsample_bytree = 0.5,
objective = "reg:linear",
eval_metric = "mae",
nthreads = 4)
set.seed(SEED)
cv_res <- xgb.cv(lParams_ensemble_xgb,
x_train,
nrounds=15000,
nfold=4,
early_stopping_rounds=150,
print_every_n = 50,
verbose= 1,
maximize=FALSE)
best_nrounds <- cv_res$best_iteration
cv_mean <- cv_res$evaluation_log$test_mae_mean[best_nrounds]
cv_stdev <- cv_res$evaluation_log$test_mae_std[best_nrounds]
print(paste("CV_Mean:", cv_mean))
print(paste("CV_Stdev:", cv_stdev))
library(tidyverse)
library(magrittr)
library(cvTools)
library(ggplot2)
library(data.table)
library(dplyr)
library(Matrix)
library(mxnet)
library(parallel)
library(readr)
library(xgboost)
ID <- 'id'
TARGET <- 'loss'
TARGET_SHIFT <- 200
SEED <- 2016
TRAIN_FILE <- "train_final.csv"
TEST_FILE <- "test_final.csv"
SUBMISSION_FILE <- "sample_submission.csv"
dfRawTrain <- read_csv(file = TRAIN_FILE)
dfRawTest <- read_csv(file = TEST_FILE)
dfRawTest$loss <- NA
ntrain <- nrow(dfRawTrain)
ntest <- nrow(dfRawTest)
dfCombined <- rbind(dfRawTrain, dfRawTest)
rm(dfRawTrain) ; rm(dfRawTest)
vFeatures <- names(dfCombined)
vFeaturesOrdinal <- c("cat74","cat78","cat79","cat85","cat87","cat90","cat101","cat102","cat103","cat105","cat111")
# Convert ordinal categorical features to lexical encoding
for(f in vFeaturesOrdinal) {
dfCombined[[f]] <- as.factor(dfCombined[[f]])
vlevels <- levels(dfCombined[[f]])
if("Other" %in% vlevels) {
idx <- which(vlevels == "Other")
vlevels <- c(vlevels[-idx],"Other")
}
dfCombined[[f]] <- factor(dfCombined[[f]], levels = vlevels)
vlevels <- levels(dfCombined[[f]])
print(paste("Ordered levels in", f, ":", paste(vlevels, collapse = ",")))
# convert to integers and then lexical encoding
dfCombined[[f]] <- as.integer(dfCombined[[f]]) - 1
dfCombined[[f]] <- dfCombined[[f]]/max(dfCombined[[f]])
}
# convert other factors to lexical encoding
for (f in vFeatures) {
if (class(dfCombined[[f]])=="character") {
levels <- sort(unique(dfCombined[[f]]))
# convert to integers and then lexical encoding
dfCombined[[f]] <- as.integer(factor(dfCombined[[f]], levels=levels)) - 1
dfCombined[[f]] <- dfCombined[[f]]/max(dfCombined[[f]])
}
}
# Split combined data set into final training and testing sets
dfTrain <- dfCombined %>% dplyr::filter(!is.na(loss))
dfTest <- dfCombined %>% dplyr::filter(is.na(loss))
rm(dfCombined)
dfPred_train <- read_csv(file = "ensemble_train_xgb_mxnet.csv")
dfPred_test <- read_csv(file = "ensemble_test_xgb_mxnet.csv")
print(paste("xgboost CV:", Metrics::mae(dfTrain$loss, dfPred_train$loss_xgboost)))
print(paste("mxnet CV:", Metrics::mae(dfTrain$loss, dfPred_train$loss_mxnet)))
y_train <- dfTrain$loss
x_train <- xgb.DMatrix(as.matrix(dfPred_train %>% select(-id)), label=y_train)
x_test <- xgb.DMatrix(as.matrix(dfPred_test %>% select(-id)))
lParams_ensemble_xgb <- list(eta = 0.001,
max_depth = 4,
min_child_weight = 1,
subsample = 0.8,
colsample_bytree = 0.5,
objective = "reg:linear",
eval_metric = "mae",
nthreads = 4)
set.seed(SEED)
cv_res <- xgb.cv(lParams_ensemble_xgb,
x_train,
nrounds=15000,
nfold=4,
early_stopping_rounds=150,
print_every_n = 50,
verbose= 1,
maximize=FALSE)
best_nrounds <- cv_res$best_iteration
cv_mean <- cv_res$evaluation_log$test_mae_mean[best_nrounds]
cv_stdev <- cv_res$evaluation_log$test_mae_std[best_nrounds]
print(paste("CV_Mean:", cv_mean))
print(paste("CV_Stdev:", cv_stdev))
head(dfPred_train)
dfPred_train$loss <- dfTrain$loss
head(dfPred_train)
write.csv(dfPred_train, file = "ensemble_train_xgb_mxnet.csv", row.names = F, quote = F)
dfPred_train <- read_csv(file = "ensemble_train_xgb_mxnet.csv")
dfPred_test <- read_csv(file = "ensemble_test_xgb_mxnet.csv")
print(paste("xgboost CV:", Metrics::mae(dfPred_train$loss, dfPred_train$loss_xgboost)))
print(paste("mxnet CV:", Metrics::mae(dfPred_train$loss, dfPred_train$loss_mxnet)))
y_train <- dfPred_train$loss
x_train <- xgb.DMatrix(as.matrix(dfPred_train %>% select(-id, -loss)), label=y_train)
x_test <- xgb.DMatrix(as.matrix(dfPred_test %>% select(-id)))
print(paste("Best_nrounds:", best_nrounds))
print(paste("CV_Mean:", cv_mean))
print(paste("CV_Stdev:", cv_stdev))
set.seed(SEED)
xgb_fit <- xgb.train(lParams_ensemble_xgb,
x_train,
nrounds = best_nrounds/0.75,
maximize = FALSE)
dfSubmission <- read_csv(file = SUBMISSION_FILE)
dfSubmission$loss <- predict(xgb_fit,x_test)
write.csv(submission,"submission_ensemble.csv",row.names = FALSE)
write.csv(dfSubmission, "submission_ensemble.csv", row.names = F, quote = F)
head(dfSubmission)
?xgb.cv
?xgb.train
source('C:/Backups/Kaggle/AllState/model_tune_xgboost.R', echo=TRUE)
library(tidyverse)
library(magrittr)
library(cvTools)
library(ggplot2)
library(data.table)
library(dplyr)
library(Matrix)
library(mxnet)
library(parallel)
library(readr)
library(xgboost)
ID <- 'id'
TARGET <- 'loss'
TARGET_SHIFT <- 200
SEED <- 2016
TRAIN_FILE <- "train_final.csv"
TEST_FILE <- "test_final.csv"
SUBMISSION_FILE <- "sample_submission.csv"
dfRawTrain <- read_csv(file = TRAIN_FILE)
dfRawTest <- read_csv(file = TEST_FILE)
dfRawTest$loss <- NA
ntrain <- nrow(dfRawTrain)
ntest <- nrow(dfRawTest)
dfCombined <- rbind(dfRawTrain, dfRawTest)
rm(dfRawTrain) ; rm(dfRawTest)
vFeatures <- names(dfCombined)
vFeaturesOrdinal <- c("cat74","cat78","cat79","cat85","cat87","cat90","cat101","cat102","cat103","cat105","cat111")
# Convert ordinal categorical features to lexical encoding
for(f in vFeaturesOrdinal) {
dfCombined[[f]] <- as.factor(dfCombined[[f]])
vlevels <- levels(dfCombined[[f]])
if("Other" %in% vlevels) {
idx <- which(vlevels == "Other")
vlevels <- c(vlevels[-idx],"Other")
}
dfCombined[[f]] <- factor(dfCombined[[f]], levels = vlevels)
vlevels <- levels(dfCombined[[f]])
print(paste("Ordered levels in", f, ":", paste(vlevels, collapse = ",")))
# convert to integers and then lexical encoding
dfCombined[[f]] <- as.integer(dfCombined[[f]]) - 1
dfCombined[[f]] <- dfCombined[[f]]/max(dfCombined[[f]])
}
# convert other factors to lexical encoding
for (f in vFeatures) {
if (class(dfCombined[[f]])=="character") {
levels <- sort(unique(dfCombined[[f]]))
# convert to integers and then lexical encoding
dfCombined[[f]] <- as.integer(factor(dfCombined[[f]], levels=levels)) - 1
dfCombined[[f]] <- dfCombined[[f]]/max(dfCombined[[f]])
}
}
# Split combined data set into final training and testing sets
dfTrain <- dfCombined %>% dplyr::filter(!is.na(loss))
dfTest <- dfCombined %>% dplyr::filter(is.na(loss))
rm(dfCombined)
df1 <- read.csv("submission_ensemble.csv")
df2 <- read.csv("submission_xgboost_avg_1111.csv")
head(df1)
head(df2)
df <- merge(df1, df2, by = intersect("id","id"), all = T)
head(df)
df <- df %>% mutate(loss = loss.x * 0.5 + loss.y * 0.5) %>% select(id, loss)
head(df)
df1 <- read.csv("submission_xgboost_avg.csv")
df2 <- read.csv("submission_xgboost_avg_1111.csv")
df <- merge(df1, df2, by = intersect("id","id"), all = T)
df <- df %>% mutate(loss = loss.x * 0.5 + loss.y * 0.5) %>% select(id, loss)
write.csv(df, file = "submission_xgboost_avg2.csv", quote = F, row.names = F)
52.27+38.7+28.51+37.43
options(scipen = 5, width = 120)
library(forcats)
library(corrplot)
library(gridExtra)
library(vcd)
library(caTools)
library(caret)
library(e1071)
library(rpart)
library(rpart.plot)
library(randomForest)
library(party)
library(ROCR)
library(plyr)
library(tidyverse)
dfRawTrain <- read.csv("train.csv", header = T)
dfRawTest <- read.csv("test.csv", header = T)
ID.VAR <- "id"
TARGET.VAR <- "loss"
dfRawTest$loss <- NA
str(dfRawTrain)
summary(dfRawTrain)
vFeatures <- setdiff(names(dfRawTrain), c(ID.VAR, TARGET.VAR))
vFeaturesType <- sapply(vFeatures, function(Z) class(dfRawTrain[,Z]))
table(vFeaturesType)
vFeaturesCat <- names(dfRawTrain)[which(sapply(dfRawTrain, is.factor))]
vFeaturesNum <- names(dfRawTrain)[which(sapply(dfRawTrain, is.numeric))]
vFeaturesNum <- setdiff(vFeaturesNum, c(ID.VAR, TARGET.VAR))
dfCombined <- rbind(dfRawTrain, dfRawTest)
sapply(names(dfCombined), function(Z) sum(is.na(dfCombined[,Z])))
dfTrain <- dfCombined %>% dplyr::filter(!is.na(loss))
dfTest <- dfCombined %>% dplyr::filter(is.na(loss)) %>% select(-loss)
# Remove combined data frame
rm(dfCombined)
rm(dfRawTrain) ; rm(dfRawTest)
install.packages("FactoMineR")
library(FactoMineR)
dfPCA <- PCA(dfTrain[vFeaturesNum])
pca_train <- dfPCA
rm(dfPCA)
pca_train$eig
length(vFeaturesNum)
vFeaturesNum
pca_train$eig
