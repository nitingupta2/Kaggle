{
    "collab_server" : "",
    "contents" : "library(data.table)\nlibrary(xgboost)\nlibrary(Matrix)\nlibrary(MASS)\nlibrary(mxnet)\n\n# define columns for categorical variables\ntake_columns <- function(file, test_file) {\n    f1 <- fread(file)\n    f2 <- fread(test_file)\n    \n    data <- rbind(f1, f2, fill = TRUE)\n    data <- data[,grep('(cat|id)', colnames(data)), with = FALSE]\n    \n    data <- melt(data, id.vars = 'id') # 2D -> 1D\n    data <- unique(data[,.(variable, value)]) # take unique columns\n    data[,variable_value := paste(variable, value, sep = \"_\")] #name columns like cat2_A\n    setorder(data, variable, value)\n    data[,n_var := .N, by = variable] # check number of values for each variable\n    data[n_var > 2,column := 1:.N] # those with number of values will be binary coded\n    data[,n_var := NULL]\n    data[,lin_val := (0:(.N-1))/(.N-1), by = variable] # all variables will be also lex coded (A - 0, B - 0.5, C - 1)\n    \n    return(data)\n}\n\n# read data\nload_data <- function(file, columns) {\n    data <- fread(file)\n    \n    # split variables between categorical and numerical\n    cn <- colnames(data)\n    c_cat <- c(\"id\", cn[grep(\"cat\", cn)]) \n    c_num <- c(cn[-grep(\"cat\", cn)])\n    \n    cat <- data[,c_cat, with = F]\n    num <- data[,c_num, with = F]\n    \n    cat <- melt(cat, id.vars = \"id\", measure.vars = c_cat[-1]) # 2D -> 1D\n    \n    rows <- cat[, .(id = unique(id))] #remember row numers for all id's\n    rows[, row := 1:.N]\n    \n    cat <- columns[cat,,on = c(\"variable\", \"value\")]\n    \n    ### assign lex coding values\n    lin_cat <- dcast(cat[,.(id, variable, lin_val)], id ~ variable, value.var = 'lin_val', fill = 0)\n    lin_cat <- lin_cat[rows[,.(id)],,on = 'id']\n    lin_cat <- Matrix(as.matrix(lin_cat[,-'id',with = FALSE]), sparse = TRUE)\n    ###\n    \n    ### assign binary coding\n    cat <- cat[!is.na(column), ]\n    cat <- rows[cat,,on = \"id\"]\n    \n    ### sparse matrix\n    cat_mat <- sparseMatrix(i = cat[,row], j = cat[,column], x = 1)\n    colnames(cat_mat) <- columns[!is.na(column),variable_value]\n    \n    num <- Matrix(as.matrix(num[,-'id',with=FALSE]), sparse = TRUE)\n    \n    ### bind all variables\n    data <- cBind(num, cat_mat, lin_cat)\n    print(\"Data loaded\")\n    return(list(data = data, rows = rows, columns = columns))\n}\n\nget_train_sample <- function(frac = 0.8, seed = 123) {\n    set.seed(seed)\n    which(runif(nrow(data)) < frac)\n}\n\ngenerate_folds = function(n_fold = 5, seed = 123, train_obs = 1:nrow(data)) {\n    set.seed(seed)\n    rnd <- sample(1:n_fold, length(train_obs), TRUE)\n    lapply(1:n_fold, function(i) which(rnd == i))\n}\n\n# box-cox\ntransform <- function(data, lambda) {\n    (data ^ lambda - 1) / lambda\n}\n\n# reverse box-cox\ndetransform <- function(data, lambda) {\n    if (is.null(lambda))\n        return(data)\n    (lambda * data + 1) ^ (1 / lambda)\n}\n\n# for xgb\nMAE <- function(pred, dtrain) {\n    pred <- detransform(pred, lambda)\n    real <- detransform(getinfo(dtrain, \"label\"), lambda)\n    return(list(metric = \"mae\", value = mean(abs(pred - real))))\n}\n\n# train one xgb model\nxgb_model <- function(train_obs, test_obs, params) {\n    train <- xgb.DMatrix(data[train_obs,-y], label = transform(data[train_obs,y], lambda))\n    test <- xgb.DMatrix(data[test_obs,-y], label = transform(data[test_obs,y], lambda))\n    \n    mod <- xgb.train(data = train,\n                     watchlist = list(test = test),\n                     params = params,\n                     nrounds = params$nrounds,\n                     early.stop.round = 50,\n                     print.every.n = 100,\n                     feval = MAE,\n                     maximize = FALSE\n    )\n    \n    pred <- detransform(predict(mod, test), lambda)\n    pred_train <- detransform(predict(mod, train), lambda)\n    \n    MAE_test <- mean(abs(pred - data[test_obs, y]))\n    MAE_train <- mean(abs(pred_train - data[train_obs, y]))\n    \n    return(list(MAE_test = MAE_test, MAE_train = MAE_train, model = mod))\n}\n\n#train one net\nnn_model <- function(train_obs, test_obs, params) {\n    inp <- mx.symbol.Variable('data')\n    l1 <- mx.symbol.FullyConnected(inp, name = \"l1\", num.hidden = 400)\n    a1 <- mx.symbol.Activation(l1, name = \"a1\", act_type = 'relu')\n    d1 <- mx.symbol.Dropout(a1, name = 'd1', p = 0.4)\n    l2 <- mx.symbol.FullyConnected(d1, name = \"l2\", num.hidden = 200)\n    a2 <- mx.symbol.Activation(l2, name = \"a2\", act_type = 'relu')\n    d2 <- mx.symbol.Dropout(a2, name = 'd2', p = 0.2)\n    l3 <- mx.symbol.FullyConnected(d2, name = \"l3\", num.hidden = 1)\n    outp <- mx.symbol.MAERegressionOutput(l3, name = \"outp\")\n    \n    m <- mx.model.FeedForward.create(outp, \n                                     X = as.array(t(data[train_obs, -y])), \n                                     y = as.array(data[train_obs, y]),\n                                     eval.data =\n                                         list(data = as.array(t(data[test_obs, -y])),\n                                              label = as.array(data[test_obs, y])),\n                                     array.layout = 'colmajor',\n                                     eval.metric=mx.metric.mae,\n                                     learning.rate = params$learning.rate,\n                                     momentum = params$momentum,\n                                     wd = params$wd,\n                                     array.batch.size = params$batch.size,\n                                     num.round = params$num.round)\n    \n    pred <- predict(m, as.array(t(data[test_obs, -y])), array.layout = 'colmajor')\n    pred_train <- predict(m, as.array(t(data[train_obs, -y])), array.layout = 'colmajor')\n    \n    MAE_test <- mean(abs(pred - data[test_obs, y]))\n    MAE_train <- mean(abs(pred_train - data[train_obs, y]))\n    \n    return(list(model = m, MAE_test = MAE_test, MAE_train = MAE_train))\n}\n\n# cross validation\ncv <- function(params = list(), param = NULL, values = NULL, folds, model = 'xgb') {\n    if (is.null(param)) {\n        param = 'evaluation!'\n        values = c(0)\n    }\n    cat(\"\\n\\nModel: \", model, \"\\n\\n\")\n    train_obs <- do.call(c, folds)\n    out <- lapply(values, function(value) {\n        par <- params\n        par[[param]] <- value\n        cat(param, \" : \", value, '\\n')\n        iter <- lapply(folds, function(fold) {\n            \n            if (model == 'xgb')\n                ret <- xgb_model(setdiff(train_obs, fold), fold, par)\n            \n            if (model == 'nn')\n                ret <- nn_model(setdiff(train_obs, fold), fold, par)\n            \n            cat('\\nMAE test: ', ret$MAE_test, 'train: ', ret$MAE_train,'\\n')\n            \n            return(ret)\n        })\n        cat('\\nMAE mean test: ', mean(sapply(iter, '[[', 'MAE_test')), 'train: ',\n            mean(sapply(iter, '[[', 'MAE_train')),\"\\n\\n\")\n        \n        return(iter)\n        \n    })\n    \n    v <- sapply(values, rep, length(folds))\n    test_data <- sapply(out, sapply, '[[', 'MAE_test')\n    train_data <- sapply(out, sapply, '[[', 'MAE_train')\n    \n    #plot something (not useful in case of evaluation)\n    plot(v, train_data, xlab = param, ylab = 'MAE train', col = 'red')\n    lines(values, sapply(out, function(l) mean(sapply(l, '[[', 'MAE_train'))), col = 'red')\n    \n    plot(v, test_data, xlab = param, ylab = 'MAE test', col = 'blue')\n    lines(values, sapply(out, function(l) mean(sapply(l, '[[', 'MAE_test'))), col = 'blue')\n    \n    \n    return(out)\n}\n\n# params for xgb\nparams <- list(\n    eta = 0.05,\n    max_depth = 8,\n    min_child_weight = 110,\n    subsample = 0.8,\n    nrounds = 1000\n)\n\n# params for nn\nparams_nn <- list(\n    learning.rate = 3e-4,\n    momentum = 0.9,\n    batch.size = 128,\n    wd = 0,\n    num.round = 60\n)\n\n#load data\ncolumns <- take_columns('train_final.csv', 'test_final.csv')\ndata_list <- load_data(\"train_final.csv\", columns)\ndata <- data_list$data\n\n# take index of response\ny <- which(colnames(data) == 'loss')\n# set cox-box parameter\nlambda = 0.3\n\n# take train sample\ntrain_obs <- get_train_sample(frac = 0.8)\n\n# train models\n# folds <- generate_folds(train_obs = train_obs)\n# out <- cv(params = params, folds = folds, model = 'xgb')\nfolds_nn <- generate_folds(train_obs = train_obs, n_fold = 3)\nout_nn <- cv(params = params_nn, folds = folds_nn, model = 'nn')\n\n\nmerge_prediction <- function(data, obs) {\n    models <- lapply(out[[1]], '[[', 'model')\n    test <- xgb.DMatrix(data[obs,-y])\n    pred <- rowMeans(sapply(models, function(m) {\n        detransform(predict(m, test), lambda)\n    }))\n    \n    return(pred)\n}\n\nmerge_prediction_nn <- function(data, obs) {\n    models_nn <- lapply(out_nn[[1]], '[[', 'model')\n    pred_nn <- rowMeans(sapply(models_nn, function(m) {\n        predict(m, as.array(t(data[obs, -y])), array.layout = 'colmajor')\n    }))\n    return(pred_nn)\n}\nprediction_both <- function(p) {\n    mean(abs(pred * p + pred_nn *(1 - p) - data[-train_obs, y]))\n}\n\n# # predict\n# pred <- merge_prediction(data, -train_obs)\npred_nn <- merge_prediction_nn(data, -train_obs)\n\n# # see how combined results perform\n# grid <- seq(0,1,0.01)\n# mae_both <- sapply(grid, prediction_both)\n# \n# p <- grid[which.min(mae_both)]\n# \n# plot(grid, mae_both, xlab = 'p', ylab = 'Combined MAE')\n",
    "created" : 1480640251208.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1204869375",
    "id" : "1FD536AE",
    "lastKnownWriteTime" : 1480717654,
    "last_content_update" : 1480717654345,
    "path" : "C:/Backups/Kaggle/AllState/model_xgboost_mxnet.R",
    "project_path" : "model_xgboost_mxnet.R",
    "properties" : {
        "tempName" : "Untitled3"
    },
    "relative_order" : 4,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}