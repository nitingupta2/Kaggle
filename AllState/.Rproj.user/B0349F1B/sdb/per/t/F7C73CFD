{
    "collab_server" : "",
    "contents" : "library(tidyverse)\nlibrary(magrittr)\nlibrary(cvTools)\nlibrary(ggplot2)\nlibrary(data.table)\nlibrary(dplyr)\nlibrary(Matrix)\nlibrary(mxnet)\nlibrary(parallel)\nlibrary(readr)\nlibrary(xgboost)\n\nID <- 'id'\nTARGET <- 'loss'\nTARGET_SHIFT <- 200\nSEED <- 2016\n\nTRAIN_FILE <- \"train_final.csv\"\nTEST_FILE <- \"test_final.csv\"\nSUBMISSION_FILE <- \"sample_submission.csv\"\n\n\ndfRawTrain <- read_csv(file = TRAIN_FILE)\ndfRawTest <- read_csv(file = TEST_FILE)\ndfRawTest$loss <- NA\n\nntrain <- nrow(dfRawTrain)\nntest <- nrow(dfRawTest)\ndfCombined <- rbind(dfRawTrain, dfRawTest)\nrm(dfRawTrain) ; rm(dfRawTest)\n\nvFeatures <- names(dfCombined)\nvFeaturesOrdinal <- c(\"cat74\",\"cat78\",\"cat79\",\"cat85\",\"cat87\",\"cat90\",\"cat101\",\"cat102\",\"cat103\",\"cat105\",\"cat111\")\n\n# Convert ordinal categorical features to lexical encoding\nfor(f in vFeaturesOrdinal) {\n    dfCombined[[f]] <- as.factor(dfCombined[[f]])\n    vlevels <- levels(dfCombined[[f]])\n    \n    if(\"Other\" %in% vlevels) {\n        idx <- which(vlevels == \"Other\")\n        vlevels <- c(vlevels[-idx],\"Other\")\n    }\n    dfCombined[[f]] <- factor(dfCombined[[f]], levels = vlevels)\n    vlevels <- levels(dfCombined[[f]])\n    \n    print(paste(\"Ordered levels in\", f, \":\", paste(vlevels, collapse = \",\")))\n    \n    # convert to integers and then lexical encoding\n    dfCombined[[f]] <- as.integer(dfCombined[[f]]) - 1\n    dfCombined[[f]] <- dfCombined[[f]]/max(dfCombined[[f]])\n}\n\n# convert other factors to lexical encoding\nfor (f in vFeatures) {\n    if (class(dfCombined[[f]])==\"character\") {\n        levels <- sort(unique(dfCombined[[f]]))\n        # convert to integers and then lexical encoding\n        dfCombined[[f]] <- as.integer(factor(dfCombined[[f]], levels=levels)) - 1\n        dfCombined[[f]] <- dfCombined[[f]]/max(dfCombined[[f]])\n    }\n}\n\n# Split combined data set into final training and testing sets\ndfTrain <- dfCombined %>% dplyr::filter(!is.na(loss))\ndfTest <- dfCombined %>% dplyr::filter(is.na(loss))\nrm(dfCombined)\n\n\n# Custom function to evaluate mean absolute error in mxnet\ncustom.metric.mae <- mx.metric.custom(\"mae\", function(label, pred) {\n    res <- Metrics::mae(exp(label),exp(pred))\n    return(res)\n})\n\n# Tuned Parameters for mxnet\nlParams_mxnet <- list(learning.rate = 1e-3,\n                      momentum = 0.9,\n                      batch.size = 100,\n                      wd = 0,\n                      num.round = 120)\n\n# Run mxnet with tuned hyper-parameters and K-folds\nmodel_mxnet <- function(dfTrain_fold, dfTest_fold) {\n    \n    x_train_fold <- data.matrix(t(dfTrain_fold %>% select(-loss, -id)))\n    x_test_fold <- data.matrix(t(dfTest_fold %>% select(-loss, -id)))\n    y_train_fold <- log(dfTrain_fold$loss + TARGET_SHIFT)\n    x_test <- data.matrix(t(dfTest %>% select(-loss, -id)))\n    \n    inp <- mx.symbol.Variable('data')\n    l1 <- mx.symbol.FullyConnected(inp, name = \"l1\", num.hidden = 200)\n    a1 <- mx.symbol.Activation(l1, name = \"a1\", act_type = 'relu')\n    d1 <- mx.symbol.Dropout(a1, name = 'd1', p = 0.2)\n    l2 <- mx.symbol.FullyConnected(d1, name = \"l2\", num.hidden = 100)\n    a2 <- mx.symbol.Activation(l2, name = \"a2\", act_type = 'relu')\n    d2 <- mx.symbol.Dropout(a2, name = 'd2', p = 0.1)\n    # l3 <- mx.symbol.FullyConnected(d2, name = \"l3\", num.hidden = 50)\n    # a3 <- mx.symbol.Activation(l3, name = \"a3\", act_type = 'relu')\n    # d3 <- mx.symbol.Dropout(a3, name = 'd3', p = 0.1)\n    l4 <- mx.symbol.FullyConnected(d2, name = \"l4\", num.hidden = 1)\n    outp <- mx.symbol.LinearRegressionOutput(l4, name = \"outp\")\n    \n    mxnet_fit <- mx.model.FeedForward.create(outp, \n                                             X = x_train_fold, \n                                             y = as.array(y_train_fold),\n                                             ctx = mx.cpu(),\n                                             eval.data = NULL,\n                                             eval.metric=custom.metric.mae,\n                                             optimizer = \"sgd\",\n                                             initializer = mx.init.uniform(0.01),\n                                             array.layout = 'colmajor',\n                                             learning.rate = lParams_mxnet$learning.rate,\n                                             momentum = lParams_mxnet$momentum,\n                                             wd = lParams_mxnet$wd,\n                                             array.batch.size = lParams_mxnet$batch.size,\n                                             num.round = lParams_mxnet$num.round)\n    \n    pred_oof <- predict(mxnet_fit, x_test_fold, array.layout = 'colmajor', \n                        array.batch.size = lParams_mxnet$batch.size, ctx = mx.cpu())\n    \n    pred_test <- predict(mxnet_fit, x_test, array.layout = 'colmajor', \n                        array.batch.size = lParams_mxnet$batch.size, ctx = mx.cpu())\n    \n    return(list(id_oof = dfTest_fold[[\"id\"]], \n                pred_oof = (exp(pred_oof) - TARGET_SHIFT), \n                pred_test = (exp(pred_test) - TARGET_SHIFT)))\n}\n\n#########################################################################################################################\nxgb_eval_mae <- function (yhat, dtrain) {\n    y = getinfo(dtrain, \"label\")\n    err= ModelMetrics::mae(exp(y),exp(yhat))\n    return (list(metric = \"mae\", value = err))\n}\n\nxgb_eval_obj <- function(preds, dtrain) { \n    con <- 2 \n    y <- getinfo(dtrain, \"label\") \n    x <- preds - y \n    grad <- con * x / (abs(x) + con) \n    hess <- con^2 / ((abs(x) + con)^2) \n    return(list(grad = grad, hess = hess)) \n}\n\nlParams_xgboost <- list(eta = 0.01,\n                        max_depth = 12,\n                        min_child_weight = 1,\n                        gamma = 2,\n                        subsample = 0.8,\n                        colsample_bytree = 0.5,\n                        alpha = 1,\n                        best_nrounds = 4857,\n                        nthreads = 4)\n\n# lParams_xgboost <- list(eta = 0.3,\n#                         max_depth = 4,\n#                         min_child_weight = 1,\n#                         gamma = 0,\n#                         subsample = 1,\n#                         colsample_bytree = 1,\n#                         alpha = 0,\n#                         best_nrounds = 381,\n#                         nthreads = 4)\n\nmodel_xgboost <- function(dfTrain_fold, dfTest_fold, numFolds, seedForFolds) {\n    \n    y_train_fold <- log(dfTrain_fold$loss + TARGET_SHIFT)\n    x_train_fold <- xgb.DMatrix(as.matrix(dfTrain_fold %>% select(-loss, -id)), label=y_train_fold)\n    x_test_fold <- xgb.DMatrix(as.matrix(dfTest_fold %>% select(-loss, -id)))\n    x_test <- xgb.DMatrix(as.matrix(dfTest %>% select(-loss, -id)))\n    \n    set.seed(seedForFolds)\n    xgb_fit <- xgb.train(lParams_xgboost,\n                         x_train_fold,\n                         feval=xgb_eval_mae,\n                         obj = xgb_eval_obj,\n                         nrounds = as.integer(lParams_xgboost$best_nrounds * (numFolds/(numFolds-1))),\n                         print_every_n = 50,\n                         maximize = FALSE)\n    \n    pred_oof <- predict(xgb_fit, x_test_fold)\n    pred_test <- predict(xgb_fit, x_test)\n    \n    return(list(id_oof = dfTest_fold[[\"id\"]], \n                pred_oof = (exp(pred_oof) - TARGET_SHIFT), \n                pred_test = (exp(pred_test) - TARGET_SHIFT)))\n}\n\n#########################################################################################################################\n\n# Create a list of models\nvModels <- c(\"xgboost\", \"mxnet\")\n\n# Set number of rounds for out of fold predictions\nnumRoundsForFolds <- 1\nnumFolds <- 5\n\n# Set primary seed for generating other seeds\nset.seed(SEED)\n# Generate seeds for creating folds\nvSeeds <- sample(10000, numRoundsForFolds)\n\nlPred_train <- list()\nlPred_test <- list()\n\nfor(m in seq_along(vModels)) {\n    modelName <- vModels[m]\n    vTrain <- rep(0, ntrain) ; names(vTrain) <- dfTrain[[\"id\"]]\n    vTest <- rep(0, ntest) ; names(vTest) <- dfTest[[\"id\"]]\n    vTestIDs <- as.character(dfTest[[\"id\"]])\n    \n    # Generate folds for different seeds\n    for(seedForFolds in vSeeds) {\n        # Generate folds for cross-validation\n        set.seed(seedForFolds)\n        lFolds <- caret::createFolds(dfTrain$loss, k = numFolds)\n        print(lapply(lFolds, summary))\n        print(lapply(lFolds, head, 15))\n        \n        for(i in 1:numFolds) {\n            print(paste(\"Training\", i, \"of\", numFolds, \"folds for model\", modelName, \"using seed\", seedForFolds))\n            idx_test <- lFolds[[i]]\n            dfTest_fold <- dfTrain[idx_test,]\n            dfTrain_fold <- dfTrain[-idx_test,]\n            \n            print(paste(\"Training fold observations:\", nrow(dfTrain_fold)))\n            print(paste(\"Test fold observations:\", nrow(dfTest_fold)))\n            \n            if(modelName == \"mxnet\") {\n                lResults <- model_mxnet(dfTrain_fold, dfTest_fold)\n            }\n            else if(modelName == \"xgboost\") {\n                lResults <- model_xgboost(dfTrain_fold, dfTest_fold, numFolds, seedForFolds)\n            }\n            vTrainIDs <- as.character(lResults$id_oof)\n            vTrain[vTrainIDs] <- vTrain[vTrainIDs] + lResults$pred_oof\n            vTest[vTestIDs] <- vTest[vTestIDs] + lResults$pred_test\n        }\n    }\n    \n    lPred_model <- list(train_loss = vTrain/numRoundsForFolds, test_loss = vTest/(numRoundsForFolds * numFolds))\n    \n    lPred_train[[m]] <- lPred_model$train_loss\n    lPred_test[[m]] <- lPred_model$test_loss\n}\n\n\ndfPred_train <- do.call(cbind.data.frame, lPred_train)\ndfPred_train <- dfPred_train %>% \n                set_colnames(paste(\"loss\", vModels, sep = \"_\")) %>% \n                rownames_to_column(var = \"id\") %>% \n                mutate(loss = dfTrain$loss)\nwrite.csv(dfPred_train, file = \"ensemble_train.csv\", row.names = F, quote = F)\n\ndfPred_test <- as.data.frame(lPred_test)\ndfPred_test <- dfPred_test %>% \n                set_colnames(paste(\"loss\", vModels, sep = \"_\")) %>% \n                rownames_to_column(var = \"id\")\nwrite.csv(dfPred_test, file = \"ensemble_test.csv\", row.names = F, quote = F)\n\n#####################################################################################################################\n################################################# ENSEMBLING ########################################################\n#####################################################################################################################\n\ndfPred_train <- read_csv(file = \"ensemble_train_xgb_mxnet.csv\")\ndfPred_test <- read_csv(file = \"ensemble_test_xgb_mxnet.csv\")\n\nprint(paste(\"xgboost CV:\", Metrics::mae(dfPred_train$loss, dfPred_train$loss_xgboost)))\nprint(paste(\"mxnet CV:\", Metrics::mae(dfPred_train$loss, dfPred_train$loss_mxnet)))\n\ny_train <- dfPred_train$loss\nx_train <- xgb.DMatrix(as.matrix(dfPred_train %>% select(-id, -loss)), label=y_train)\nx_test <- xgb.DMatrix(as.matrix(dfPred_test %>% select(-id)))\n\nlParams_ensemble_xgb <- list(eta = 0.001,\n                             max_depth = 4,\n                             min_child_weight = 1,\n                             subsample = 0.8,\n                             colsample_bytree = 0.5,\n                             objective = \"reg:linear\",\n                             eval_metric = \"mae\",\n                             nthreads = 4)\n\nset.seed(SEED)\ncv_res <- xgb.cv(lParams_ensemble_xgb,\n                 x_train,\n                 nrounds=15000,\n                 nfold=4,\n                 early_stopping_rounds=150,\n                 print_every_n = 50,\n                 verbose= 1,\n                 maximize=FALSE)\n\nbest_nrounds <- cv_res$best_iteration\ncv_mean <- cv_res$evaluation_log$test_mae_mean[best_nrounds]\ncv_stdev <- cv_res$evaluation_log$test_mae_std[best_nrounds]\n\nprint(paste(\"Best_nrounds:\", best_nrounds))\nprint(paste(\"CV_Mean:\", cv_mean))\nprint(paste(\"CV_Stdev:\", cv_stdev))\n\nset.seed(SEED)\nxgb_fit <- xgb.train(lParams_ensemble_xgb,\n                     x_train,\n                     nrounds = best_nrounds/0.75,\n                     maximize = FALSE)\n\ndfSubmission <- read_csv(file = SUBMISSION_FILE)\ndfSubmission$loss <- predict(xgb_fit,x_test)\nwrite.csv(dfSubmission, \"submission_ensemble.csv\", row.names = F, quote = F)\n",
    "created" : 1481213011612.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2036121543",
    "id" : "F7C73CFD",
    "lastKnownWriteTime" : 1481410263,
    "last_content_update" : 1481410263774,
    "path" : "C:/Backups/Kaggle/AllState/model_ensemble.R",
    "project_path" : "model_ensemble.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 6,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}