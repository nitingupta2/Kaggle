{
    "collab_server" : "",
    "contents" : "# # Install xgboost 0.6 from github\n# xgb_version <- packageVersion(\"xgboost\")\n# if(xgb_version<\"0.6.0\") source(\"install_xgboost.R\")\n\nif(!require(Metrics)) install.packages(\"Metrics\")\nif(!require(ModelMetrics)) install.packages(\"ModelMetrics\")\n\nlibrary(caret)\nlibrary(ggplot2)\nlibrary(data.table)\nlibrary(dplyr)\nlibrary(Matrix)\nlibrary(Metrics)\nlibrary(xgboost)\nlibrary(parallel)\n\nID <- 'id'\nTARGET <- 'loss'\nTARGET_SHIFT <- 200\nSEED <- 2016\n\nTRAIN_FILE <- \"train_final.csv\"\nTEST_FILE <- \"test_final.csv\"\nSUBMISSION_FILE <- \"sample_submission.csv\"\n\n\ntrain = fread(TRAIN_FILE, showProgress = TRUE)\ntest = fread(TEST_FILE, showProgress = TRUE)\n\ny_train = log(train[,TARGET, with = FALSE] + TARGET_SHIFT)[[TARGET]]\n\ntrain[, c(ID, TARGET) := NULL]\ntest[, c(ID) := NULL]\n\nntrain <- nrow(train)\ntrain_test <- rbind(train, test)\nrm(train) ; rm(test)\n\nvFeatures <- names(train_test)\nvFeaturesOrdinal <- c(\"cat74\",\"cat78\",\"cat79\",\"cat85\",\"cat87\",\"cat90\",\"cat101\",\"cat102\",\"cat103\",\"cat105\",\"cat111\")\n\n# Convert ordinal categorical features to integers\nfor(f in vFeaturesOrdinal) {\n    train_test[[f]] <- as.factor(train_test[[f]])\n    vlevels <- levels(train_test[[f]])\n    \n    if(\"Other\" %in% vlevels) {\n        idx <- which(vlevels == \"Other\")\n        vlevels <- c(vlevels[-idx],\"Other\")\n    }\n    train_test[[f]] <- factor(train_test[[f]], levels = vlevels)\n    vlevels <- levels(train_test[[f]])\n\n    print(paste(\"Ordered levels in\", f, \":\", paste(vlevels, collapse = \",\")))\n    \n    # convert to integers\n    train_test[[f]] <- as.integer(train_test[[f]])\n}\n\n# convert other factors to integers\nfor (f in vFeatures) {\n    if (class(train_test[[f]])==\"character\") {\n        levels <- sort(unique(train_test[[f]]))\n        train_test[[f]] <- as.integer(factor(train_test[[f]], levels=levels))\n    }\n}\n\nx_train <- train_test[1:ntrain,]\nx_test <- train_test[(ntrain+1):nrow(train_test),]\n\ndtrain = xgb.DMatrix(as.matrix(x_train), label=y_train)\ndtest = xgb.DMatrix(as.matrix(x_test))\n\n\nxg_eval_mae <- function (yhat, dtrain) {\n    y = getinfo(dtrain, \"label\")\n    err= ModelMetrics::mae(exp(y),exp(yhat))\n    return (list(metric = \"mae\", value = err))\n}\n\nxg_eval_obj <- function(preds, dtrain) { \n    fair_constant <- 2 \n    y <- getinfo(dtrain, \"label\") \n    x <- preds - y \n    grad <- fair_constant * x / (abs(x) + fair_constant) \n    hess <- fair_constant^2 / ((abs(x) + fair_constant)^2) \n    return(list(grad = grad, hess = hess)) \n}\n\n\ndef_eta <- 0.3\ndef_max_depth <- 6\ndef_min_child_weight <- 1\ndef_gamma <- 0\ndef_subsample <- 1\ndef_colsample_bytree <- 1\ndef_alpha <- 0\n\ndfParams <- expand.grid(eta=0.03, max_depth=12, min_child_weight=100,\n                        gamma=def_gamma, subsample=0.7, colsample_bytree=0.7,\n                        alpha=def_alpha)\n\n# dfParams <- expand.grid(eta=def_eta, max_depth=c(3,4,6,8,10,12), min_child_weight=def_min_child_weight,\n#                         gamma=def_gamma, subsample=def_subsample, colsample_bytree=def_colsample_bytree,\n#                         alpha=def_alpha)\nprint(dfParams)\n\nfor(i in 1:nrow(dfParams)) {\n    print(paste(\"Tuning parameter combination\", i, \"of\", nrow(dfParams)))\n    \n    eta = dfParams$eta[i]\n    max_depth = dfParams$max_depth[i]\n    min_child_weight = dfParams$min_child_weight[i]\n    gamma = dfParams$gamma[i]\n    subsample = dfParams$subsample[i]\n    colsample_bytree = dfParams$colsample_bytree[i]\n    alpha = dfParams$alpha[i]\n    \n    xgb_params = list(\n                    colsample_bytree = colsample_bytree,\n                    subsample = subsample,\n                    eta = eta,\n                    max_depth = max_depth,\n                    min_child_weight = min_child_weight,\n                    alpha = alpha,\n                    gamma = gamma,\n                    nthreads = 4\n    )\n    \n    start_time <- Sys.time()\n    \n    set.seed(SEED)\n    cv_res <- xgb.cv(xgb_params,\n                     dtrain,\n                     nrounds=15000,\n                     nfold=10,\n                     early_stopping_rounds=ifelse(eta < 0.01,150,50),\n                     print_every_n = 50,\n                     verbose= 1,\n                     feval=xg_eval_mae,\n                     obj=xg_eval_obj,\n                     maximize=FALSE)\n    \n    best_nrounds <- cv_res$best_iteration\n    cv_mean <- cv_res$evaluation_log$test_mae_mean[best_nrounds]\n    cv_std <- cv_res$evaluation_log$test_mae_std[best_nrounds]\n    \n    end_time <- Sys.time()\n    \n    dfCV <- data.frame(cv_mean=cv_mean, cv_std=cv_std, min_taken=as.numeric(difftime(end_time, start_time, units = \"min\")),\n                       best_iteration=best_nrounds, eta=eta, max_depth=max_depth, min_child_weight=min_child_weight,\n                       gamma=gamma, subsample=subsample, colsample_bytree=colsample_bytree,\n                       alpha=alpha)\n    \n    cvFileName <- \"cv_xgboost.csv\"\n    if(file.exists(cvFileName)) {\n        dfCV_all <- read.csv(cvFileName, header = T)\n        dfCV <- rbind(dfCV_all, dfCV)\n    }\n    write.csv(dfCV, file = cvFileName, row.names = F)\n}\n\nprint(dfCV %>% arrange(desc(cv_mean)))\n\n# set.seed(SEED)\n# xgb_fit <- xgb.train(xgb_params, \n#                      dtrain, \n#                      feval=xg_eval_mae,\n#                      obj = xg_eval_obj,\n#                      nrounds = best_nrounds/0.75,\n#                      maximize = FALSE)\n# \n# submission <- fread(SUBMISSION_FILE, colClasses = c(\"integer\", \"numeric\"))\n# submission$loss <- exp(predict(xgb_fit,dtest)) - TARGET_SHIFT\n# write.csv(submission,\"submission_xgboost.csv\",row.names = FALSE)\n",
    "created" : 1480664849889.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "4033999697",
    "id" : "CC421280",
    "lastKnownWriteTime" : 1481414031,
    "last_content_update" : 1481414031198,
    "path" : "C:/Backups/Kaggle/AllState/model_tune_xgboost.R",
    "project_path" : "model_tune_xgboost.R",
    "properties" : {
    },
    "relative_order" : 7,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}