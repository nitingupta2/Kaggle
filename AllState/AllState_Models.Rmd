---
title: "AllState_Models"
author: "Nitin Gupta"
date: "October 19, 2016"
output: html_document
---

```{r LoadLibs, echo=FALSE, message=FALSE, warning=FALSE}
rm(list = ls())
options(scipen = 5)
library(corrplot)
library(gridExtra)
library(vcd)
library(caTools)
library(caret)
library(e1071)
library(rpart)
library(rpart.plot)
library(randomForest)
library(party)
library(ROCR)
library(plyr)
library(tidyverse)

# Set number of cores for parallel computation in caret
library(doParallel)
registerDoParallel(cores = 4)
```



```{r ReadRDS, echo=FALSE, message=FALSE, warning=FALSE}
TARGET.VAR <- "loss"
vOutcomes <- readRDS("vOutcomes.rds")
# Convert outcomes to log format
vOutcomes <- log(vOutcomes)

# Predictors
vPredictors <- readRDS("vFeatures.Dummified.rds")

# Dummified training set
dfTrainDummified <- readRDS("dfTrain.Dummified.rds")

# Dummified test set
dfTestDummified <- readRDS("dfTest.Dummified.rds")

# Read test ids
vTestIDs <- readRDS("vTestIDs.rds")
```



```{r LinearModel, echo=FALSE, message=FALSE, warning=FALSE}
dfTrain_lm <- cbind(dfTrainDummified, logloss = vOutcomes)
model_lm <- lm(logloss ~ ., data = dfTrain_lm)
ModelMetrics::rmse(dfTrain_lm$logloss, exp(model_lm$fitted.values))
summary(model_lm)

pred_lm <- predict(model_lm, newdata = dfTestDummified)
dfPred_lm <- data.frame(id=vTestIDs, loss=exp(pred_lm), stringsAsFactors = F)
write.csv(dfPred_lm, file = "submission_lm.csv", quote=F, row.names = F)
rm(dfPred_lm)
```



```{r TrainingModels, echo=FALSE, message=FALSE, warning=FALSE}
### hyperparms -------------------

cv.folds <- 5
cv.repeats <- 5
tuneLength.set <- 5

### set seed values -------------------
set.seed(321)
seeds <- vector(mode = "list", length = (cv.folds*cv.repeats +1))
for(i in 1:(cv.folds*cv.repeats)) seeds[[i]] <- sample.int(100000, tuneLength.set)
seeds[[cv.folds*cv.repeats +1]] <- 456                                 ### final model

### Use repeated CV -------------------
ctrl <- trainControl(method = "repeatedcv",
                     number = cv.folds,
                     repeats = cv.repeats,
                     seeds = seeds,
                     allowParallel = TRUE)

##### models -------------------

set.seed(12345)
model_glmnet <- train(x = dfTrainDummified, y = vOutcomes, trControl=ctrl, method="glmnet", tuneLength=tuneLength.set)

set.seed(12345)
model_rf <- train(x = dfTrainDummified, y = vOutcomes, trControl=ctrl, method="rf")


##### out-of-sample accuracy  -------------------

cv.perf <- resamples(list(glmnet=model_glmnet, rf=model_rf))
summary(cv.perf)
dotplot(cv.perf, metric="RMSE")
```


# Test set predictions

```{r Predictions, echo=FALSE, message=FALSE, warning=FALSE}

# Predict glmnet
pred_glmnet <- predict(model_glmnet, newdata = dfTestDummified)
pred_glmnet <- exp(pred_glmnet)
saveRDS(pred_glmnet, "pred_glmnet.rds")
dfPred_glmnet <- data.frame(id=vTestIDs, loss=pred_glmnet, stringsAsFactors = F)
write.csv(dfPred_glmnet, file = "submission_glmnet.csv", quote=F, row.names = F)
rm(dfPred_glmnet)

# Predict rf
pred_rf <- predict(model_rf, newdata = dfTestDummified)
pred_rf <- exp(pred_rf)
saveRDS(pred_rf, "pred_rf.rds")
dfPred_rf <- data.frame(id=vTestIDs, loss=pred_rf, stringsAsFactors = F)
write.csv(dfPred_rf, file = "submission_rf.csv", quote=F, row.names = F)
rm(dfPred_rf)
```



### Parameter Tuning by K-fold Cross Validation

```{r Best Tuning Parameters}

getBestTune <- function(modelMethod, vOutcomes, dfTrainDummified, fitControl) {
    y <- vOutcomes
    x <- dfTrainDummified

    print(paste("Tuning parameters for", modelMethod))
    
    if(modelMethod == "gbm") {
        set.seed(2300)
        model_obj <- train(x, y, method = modelMethod, trControl = fitControl, verbose = F)
    }
    else if(modelMethod == "rpart") {
        rpartGrid <- expand.grid(cp = seq(0.001, 0.1, 0.001))
        model_obj <- train(x, y, method = modelMethod, trControl = fitControl, tuneGrid = rpartGrid)
    }
    else if(modelMethod == "rf") {
#         rfGrid <- expand.grid(mtry = c(2, 3, 4))
        set.seed(2300)
        model_obj <- train(x, y, method = modelMethod, importance = F, nodesize = 50, 
                           trControl = fitControl)
    }
    else {
        set.seed(2300)
        model_obj <- train(x, y, method = modelMethod, trControl = fitControl)
    }
    return(model_obj)
}

vModelMethods <- c("glmnet", "gbm", "lda", "rf", "rpart", "svmRadial", "xgbLinear")

# Set trainControl for cross-validation
fitControl <- trainControl(method = "cv", number = 5, allowParallel = T)

lBestTune <- list()
for(i in seq_along(vModelMethods)) {
    modelMethod <- vModelMethods[i]
    lBestTune[[i]] <- getBestTune(modelMethod, dfTrain[[TARGET.VAR]], dfTrainDummified, fitControl)
}
names(lBestTune) <- vModelMethods

# Check cross-validation accuracy
cv.perf <- resamples(lBestTune)
dotplot(cv.perf, metric = "mae")
summary(cv.perf)
```
