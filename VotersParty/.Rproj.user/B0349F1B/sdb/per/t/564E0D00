{
    "collab_server" : "",
    "contents" : "---\ntitle: \"VotersParty\"\nauthor: \"Nitin Gupta\"\ndate: \"June 13, 2016\"\noutput: html_document\n---\n\n\n### Dataset features\n\n* USER_ID - an anonymous id unique to a given user\n* YOB - the year of birth of the user\n* Gender - the gender of the user, either Male or Female\n* Income - the household income of the user. Either not provided, or one of \"under $25,000\", \"$25,001 - $50,000\", \"$50,000 - $74,999\", \"$75,000 - $100,000\", \"$100,001 - $150,000\", or \"over $150,000\".\n* HouseholdStatus - the household status of the user. Either not provided, or one of \"Domestic Partners (no kids)\", \"Domestic Partners (w/kids)\", \"Married (no kids)\", \"Married (w/kids)\", \"Single (no kids)\", or \"Single (w/kids)\".\n* EducationalLevel - the education level of the user. Either not provided, or one of \"Current K-12\", \"High School Diploma\", \"Current Undergraduate\", \"Associate's Degree\", \"Bachelor's Degree\", \"Master's Degree\", or \"Doctoral Degree\".\n* Party - the political party for whom the user intends to vote for. Either \"Democrat\" or \"Republican\n* Q124742, Q124122, . . . , Q96024 - 101 different questions that the users were asked on Show of Hands. If the user didn't answer the question, there is a blank. For information about the question text and possible answers, see the file Questions.pdf.\n\n__Note to Self__: In many cases, the demographic data is not provided or misstated (YOB).\nHow to deal with this?\n\n* Read everything as character\n* Treat missing survey data as 'dnr' (did not respond)\n* Change survey data to factors\n* Impute demographic data and then convert to factors\n\n### Read Data\n\n```{r LoadData, message=FALSE, warning=FALSE}\noptions(scipen = 5, width = 100)\nlibrary(magrittr)\nlibrary(tidyr)\nlibrary(plyr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(vcd)\nlibrary(caTools)\nlibrary(caret)\nlibrary(e1071)\nlibrary(rpart)\nlibrary(rpart.plot)\nlibrary(randomForest)\nlibrary(party)\nlibrary(ROCR)\nsource(\"C:\\\\Backups\\\\Dropbox\\\\Dev\\\\R\\\\Source\\\\Func_corProb.R\")\nsource(\"C:\\\\Backups\\\\Dropbox\\\\Dev\\\\R\\\\Source\\\\Func_flattenSquareMatrix.R\")\nsource(\"C:\\\\Backups\\\\Dropbox\\\\Dev\\\\R\\\\Source\\\\Func_getModelCharacteristics.R\")\n\ndfQuestions <- read.delim(\"questions.tsv\") %>% set_colnames(c(\"ID\", \"QnA\"))\ndfQuestions$ID <- paste0(\"Q\", dfQuestions$ID)\n\ndfRawTrain <- read.csv(\"train2016.csv\", na.strings = c(\"\", NA))\ndfRawTest <- read.csv(\"test2016.csv\", na.strings = c(\"\", NA))\ndfRawTest$Party <- NA\n\nnames(dfRawTrain)\nvFeaturesDemographic <- c(\"Party\", \"YOB\", \"Gender\", \"Income\", \"HouseholdStatus\", \"EducationLevel\")\nvFeaturesSurveyQues <- names(dfRawTrain)[!(names(dfRawTrain) %in% vFeaturesDemographic)]\n# exclude USER_ID from survey questions\nvFeaturesSurveyQues <- vFeaturesSurveyQues[-1]\n\n# Summaries of demographic features\nsummary(dfRawTrain[vFeaturesDemographic])\nsummary(dfRawTest[vFeaturesDemographic])\n```\n\n\n### Exploratory Data Analysis\n\n\n```{r EDA}\n# Missing data by USER_ID ordered from both training and test sets\ndfCombined <- rbind(dfRawTrain, dfRawTest) %>% arrange(USER_ID)\nAmelia::missmap(dfCombined, \n                main=\"Missing Map\", col=c(\"yellow\",\"black\"), legend=F, \n                x.cex = 0.6, y.cex = 0.5, y.lab=F, y.at=F, rank.order = F)\n# Missing data by number of missing survey responses\ndfCombined$NumMissingSurvey <- apply(dfCombined[vFeaturesSurveyQues], 1, function(Z) sum(is.na(Z)))\ndfCombined <- dfCombined %>% arrange(desc(NumMissingSurvey))\nAmelia::missmap(dfCombined, \n                main=\"Missing Map\", col=c(\"yellow\",\"black\"), legend=F, \n                x.cex = 0.6, y.cex = 0.5, y.lab=F, y.at=F, rank.order = F)\nrm(dfCombined)\n\n# Histograms\n######################################################################################################\nggplot(dfRawTrain, aes(x = YOB, fill = Party)) +\n    geom_histogram(binwidth = 10, color = \"black\") +\n    scale_x_continuous(limits = c(1930, 2003)) +\n    scale_fill_manual(values = c(\"blue\",\"red\"))\nsummary(dfRawTrain$YOB)\n# Median YOB by gender: Female 1983, Male 1982\ntapply(dfRawTrain$YOB, dfRawTrain$Gender, median, na.rm = T)\n\n# Many outliers and anomalies. For model building exclude YOB > 2003 and YOB < 1930\nsummary(subset(dfRawTrain[vFeaturesDemographic], YOB > 2003))\nsummary(subset(dfRawTrain[vFeaturesDemographic], YOB < 1930))\n        \nggplot(dfRawTest, aes(x = YOB)) +\n    geom_histogram(binwidth = 1, color = \"black\", fill = \"white\") \nsummary(dfRawTest$YOB)\nsubset(dfRawTest[vFeaturesDemographic], YOB < 1930)\n\n# Boxplots\n######################################################################################################\nggplot(dfRawTrain, aes(x = Party, y = YOB, fill = Party)) +\n    geom_boxplot(position = \"dodge\") +\n    scale_fill_manual(values = c(\"blue\", \"red\"))\n# Nothing remarkable here\n\nggplot(dfRawTrain, aes(x = Gender, y = YOB, fill = Party)) +\n    geom_boxplot(position = \"dodge\") +\n    scale_fill_manual(values = c(\"blue\", \"red\"))\n# Nothing remarkable here\n\nggplot(dfRawTrain, aes(x = HouseholdStatus, y = YOB, fill = Party)) +\n    geom_boxplot(position = \"dodge\") +\n    scale_fill_manual(values = c(\"blue\", \"red\")) +\n    theme(axis.text.x = element_text(angle = 90, hjust = 1))\n# Median ages of voters of status 'Single (no kids)' and NA have significant differences with the rest\n# This indicates it could be a useful predictor\n# Median ages of voters indicated by 'Single (no kids)' is ~ same as those who have not provided an answer\n# Assign 'Single (no kids)' to HouseholdStatus NA\n\nggplot(dfRawTrain, aes(x = EducationLevel, y = YOB, fill = Party)) +\n    geom_boxplot(position = \"dodge\") +\n    scale_fill_manual(values = c(\"blue\", \"red\")) +\n    theme(axis.text.x = element_text(angle = 90, hjust = 1))\ntable(dfRawTrain$EducationLevel, dfRawTrain$Party)\n# As expected, median ages of Current K-12 and Current Undergraduate are significantly different from the rest\n# But these groups have very small sample sizes relative to the rest\n# People who didn't provide EducationLevel are more likely to be dropouts. Label EducationLevel NA as 'dropout'\n\nggplot(dfRawTrain, aes(x = Income, y = YOB, fill = Party)) +\n    geom_boxplot(position = \"dodge\") +\n    scale_fill_manual(values = c(\"blue\", \"red\")) +\n    theme(axis.text.x = element_text(angle = 90, hjust = 1))\ntable(dfRawTrain$EducationLevel, dfRawTrain$Income, useNA = \"ifany\")\n# Where Income is NA, assign Income by the category in which the maximum number of voters fall by EducationLevel\n\n# Relationship between 3 categorical variables\nggplot(dfRawTrain, aes(x = Income, fill = Party)) + \n    geom_bar() + \n    facet_grid(EducationLevel~Gender, scales = \"free_y\") + \n    scale_fill_manual(values = c(\"blue\",\"red\"))\n\nggplot(dfRawTrain, aes(x = Q109244, fill = Party)) + \n    geom_bar() + \n    facet_grid(Q101163~Q113181, scales = \"free_y\") + \n    scale_fill_manual(values = c(\"blue\",\"red\"))\n\n\n# Heatmaps\n######################################################################################################\nggplot(dfRawTrain, aes(x = EducationLevel, y = Income)) +\n    geom_tile(aes(fill = YOB)) +\n    scale_fill_gradient(low=\"yellow\", high=\"cyan\", limits = c(1930, 2003))\ntable(dfRawTrain$EducationLevel, dfRawTrain$Income, useNA = \"ifany\")\n# Voters with Masters degrees earning b/w $25,000 - $50,000 were born in 1940s => Retirees on Social Security income\n# High income voters with Associate degrees are older \n\nggplot(dfRawTrain, aes(x = EducationLevel, y = HouseholdStatus)) +\n    geom_tile(aes(fill = YOB)) +\n    scale_fill_gradient(low=\"yellow\", high=\"cyan\", limits = c(1930, 2003))\ntable(dfRawTrain$EducationLevel, dfRawTrain$HouseholdStatus, useNA = \"ifany\")\n# Voters with Masters degrees earning b/w $25,000 - $50,000 were born in 1940s\n# High income voters with Associate degrees are older \n\nggplot(dfRawTrain, aes(x = HouseholdStatus, y = Income)) +\n    geom_tile(aes(fill = YOB)) +\n    scale_fill_gradient(low=\"yellow\", high=\"cyan\", limits = c(1930, 2003))\ntable(dfRawTrain$HouseholdStatus, dfRawTrain$Income, useNA = \"ifany\")\n\n# Mosaic Plots\n######################################################################################################\nwith(dfRawTrain, mosaicplot(Gender ~ Party, shade=F, color=c(\"blue\",\"red\"), \n                         xlab=\"Gender\", ylab=\"Party\", main=\"Voting by Gender\"))\ntable(dfRawTrain$Gender, dfRawTrain$Party)\n# Larger proportion of Males than Females\n# Males vote Republican just slighly more than Democrat\n# Females vote Democrat significantly more than Republican\n\nwith(dfRawTrain, mosaicplot(HouseholdStatus ~ Party, shade=F, color=c(\"blue\",\"red\"), \n                         xlab=\"HouseholdStatus\", ylab=\"Party\", main=\"Voting by Household Status\"))\ntable(dfRawTrain$HouseholdStatus, dfRawTrain$Party)\n\nwith(dfRawTrain, mosaicplot(EducationLevel ~ Party, shade=F, color=c(\"blue\",\"red\"), \n                         xlab=\"EducationLevel\", ylab=\"Party\", main=\"Voting by Education Level\"))\n\nwith(dfRawTrain, mosaicplot(Income ~ Party, shade=F, color=c(\"blue\",\"red\"), \n                         xlab=\"Income\", ylab=\"Party\", main=\"Voting by Income Range\"))\ntable(dfRawTrain$Income, dfRawTrain$Party)\n\n# Prediction accuracy of Survey questions\ndfSurveyAccuracy <- data.frame()\nfor(i in 1:length(vFeaturesSurveyQues)) {\n    tbl <- table(dfRawTrain[c(\"Party\", vFeaturesSurveyQues[i])])\n    GOP_accuracy <- round(sum(diag(tbl))/sum(tbl), digits = 4)\n    DEM_accuracy <- round((tbl[1,2] + tbl[2,1])/sum(tbl), digits = 4)\n    dfSurveyAccuracy = rbind(dfSurveyAccuracy, data.frame(ques = vFeaturesSurveyQues[i], GOP_accuracy, DEM_accuracy))\n}\nrm(GOP_accuracy) ; rm(DEM_accuracy) ; rm(tbl)\ndfSurveyAccuracy <- dfSurveyAccuracy %>% arrange(desc(GOP_accuracy))\nhead(dfSurveyAccuracy, 10)\ndfSurveyAccuracy <- dfSurveyAccuracy %>% arrange(desc(DEM_accuracy))\nhead(dfSurveyAccuracy, 10)\n```\n\n\n### Preprocessing and Feature Engineering\n\n```{r Preprocessing, echo=FALSE, message=FALSE, warning=FALSE}\n# Merge training and test sets\ndfMerged <- rbind(dfRawTrain, dfRawTest)\n\n# With chisq goodness of fit determine survey questions whose probability distribution \n# is very different from Party distribution in the data set\ndfXsq <- data.frame()\nfor(i in 1:length(vFeaturesSurveyQues)) {\n    indexQ <- which(names(dfMerged) == vFeaturesSurveyQues[i])\n    tbl <- table(dfMerged$Party, dfMerged[,indexQ])\n    Xsq <- chisq.test(tbl)\n    dfXsq <- rbind(dfXsq, data.frame(surveyQues = vFeaturesSurveyQues[i], pvalue = Xsq$p.value))\n}\ndfXsq <- dfXsq %>% dplyr::filter(pvalue < 0.001) %>% arrange(pvalue)\nvFeaturesSurveyInterest <- as.character(dfXsq$surveyQues)\nlapply(dfRawTrain[vFeaturesSurveyInterest], function(x) table(unlist(x), unlist(dfRawTrain[[\"Party\"]]), useNA = \"ifany\"))\n\n# Since many respondents have not provided demographic and survey questions, # of valid responses need to be recorded\ndfMerged$NumAnsS <- apply(dfMerged[vFeaturesSurveyQues], 1, function(Z) sum(!is.na(Z)))\ndfMerged$NumAnsP <- apply(dfMerged[vFeaturesSurveyInterest], 1, function(Z) sum(!is.na(Z)))\n\n# See missing map ordered by number of responses to survey questions of interest\nsummary(dfMerged$NumAnsP)\nAmelia::missmap(dfMerged[c(\"USER_ID\", \"NumAnsP\", vFeaturesDemographic, vFeaturesSurveyInterest)] %>%\n                    arrange(desc(NumAnsP)), \n                main=\"Missing Map\", col=c(\"yellow\",\"black\"), legend=F, \n                x.cex = 0.6, y.cex = 0.5, y.lab=F, y.at=F, rank.order = F)\n\n# Assign 'dnr' to unanswered survey questions and convert to factors\ndfMerged[vFeaturesSurveyQues] <- lapply(dfMerged[vFeaturesSurveyQues], as.character)\ndfMerged[vFeaturesSurveyQues][is.na(dfMerged[vFeaturesSurveyQues])] <- \"dnr\"\ndfMerged[vFeaturesSurveyQues] <- lapply(dfMerged[vFeaturesSurveyQues], as.factor)\ndfMerged[vFeaturesSurveyQues] <- lapply(dfMerged[vFeaturesSurveyQues], relevel, \"dnr\")\nsummary(dfMerged[c(1:9)])\n\n# Q113181 & Q98197 (Do you pray or meditate?) ask the same thing\n# Some missing responses in Q113181 could be imputed from Q98197\ndfMerged <- dfMerged %>% \n            mutate(Q113181 = as.character(Q113181),\n                   Q98197 = as.character(Q98197)) %>% \n            mutate(Q113181 = ifelse(Q113181 == \"dnr\", Q98197, Q113181),\n                   Q113181 = as.factor(Q113181),\n                   Q98197 = as.factor(Q98197))\n\n# Assign 'Single (no kids)' to HouseholdStatus NA\ndfMerged <- dfMerged %>% \n            mutate(HouseholdStatus = as.character(HouseholdStatus),\n                   HouseholdStatus = ifelse(is.na(HouseholdStatus), \"Single (no kids)\", HouseholdStatus),\n                   HouseholdStatus = as.factor(HouseholdStatus)) %>% \n            mutate(HouseholdStatus = factor(HouseholdStatus, levels = c(\"Single (no kids)\", \n                                                                        \"Married (no kids)\",\n                                                                        \"Domestic Partners (no kids)\",\n                                                                        \"Single (w/kids)\", \n                                                                        \"Married (w/kids)\",\n                                                                        \"Domestic Partners (w/kids)\")))\n# Assign 'dnr' to EducationLevel NA\ndfMerged <- dfMerged %>% \n            mutate(EducationLevel = as.character(EducationLevel),\n                   EducationLevel = ifelse(is.na(EducationLevel), \"dnr\", EducationLevel),\n                   EducationLevel = as.factor(EducationLevel)) %>% \n            mutate(EducationLevel = factor(EducationLevel, \n                                           levels = c(\"dnr\", \"Current K-12\", \"High School Diploma\", \n                                                      \"Associate's Degree\", \"Current Undergraduate\", \n                                                      \"Bachelor's Degree\", \"Master's Degree\", \"Doctoral Degree\")))\n# Assign \"dnr\" to Income NA\ndfMerged <- dfMerged %>% \n            mutate(Income = as.character(Income),\n                   Income = ifelse(is.na(Income), \"dnr\", Income),\n                   Income = as.factor(Income)) %>% \n            mutate(Income = factor(Income, levels = c(\"dnr\", \"under $25,000\", \"$25,001 - $50,000\", \"$50,000 - $74,999\",\n                                                      \"$75,000 - $100,000\", \"$100,001 - $150,000\", \"over $150,000\")))\n\n# Assign \"dnr\" to Gender NA\ndfMerged <- dfMerged %>% \n            mutate(Gender = as.character(Gender),\n                   Gender = ifelse(is.na(Gender), \"dnr\", Gender),\n                   Gender = as.factor(Gender)) %>% \n            mutate(Gender = factor(Gender, levels = c(\"dnr\", \"Female\", \"Male\")))\n\n# Reduce number of features from final data frame\ndfMerged <- dfMerged[c(\"USER_ID\", \"NumAnsP\", \"NumAnsS\", vFeaturesDemographic, vFeaturesSurveyInterest)]\nsummary(dfMerged)\n\n# Impute missing YOB values by MICE\n#####################################################################################################################\n# 1. Exclude outlier YOB values before imputing\ndfNonImputed <- dfMerged %>% dplyr::filter(YOB < 1930 | YOB > 2003)\ndfImputed <- subset(dfMerged, !USER_ID %in% dfNonImputed$USER_ID)\nvFeaturesImputed <- vFeaturesDemographic[-1]\n\n# 2. Impute missing demographics\nlibrary(mice)\nset.seed(2300)\ndfImputed[vFeaturesImputed] <- complete(mice(dfImputed[vFeaturesImputed]))\n\n# 3. Recombine non-imputed and imputed sets and relevel factors\ndfMerged <- rbind(dfNonImputed, dfImputed) %>% \n            arrange(USER_ID)\n\nsummary(dfMerged)\nstr(dfMerged)\nAmelia::missmap(dfMerged, \n                main=\"Missing Map\", col=c(\"yellow\",\"black\"), legend=F, \n                x.cex = 0.6, y.cex = 0.5, y.lab=F, y.at=F, rank.order = F)\n\n# Feature Engineering\n#####################################################################################################################\n\n# Create Age_groups\ndfMerged <- dfMerged %>% \n            mutate(Age = 2013 - YOB) %>% \n            mutate(Age_group = cut(Age, \n                                   breaks = c(min(Age, na.rm = T), 17, 29, 44, 60, max(Age, na.rm = T)),\n                                   labels = c(\"below 18\", \"18-29\", \"30-44\", \"45-60\", \"above 60\"),\n                                   include.lowest = T))\n\nsummary(dfMerged)\n```\n\n\n### Split Data\n\n```{r SplitData}\n# Split original data from merged data after imputation and excluding YOB outliers\n# dfTrain <- subset(dfMerged, !is.na(Party) & (YOB >= 1930 & YOB <= 2003) & (NumAnsP >= quantile(NumAnsP, 0.25)))\n#================================================================================================================\n# Doesn't help to exclude too much data where survey responses are not provided\n# The models have much better accuracy 0.66 (glm) on dfModelTest but do not generalize well on dfTest\n#================================================================================================================\ndfTrain <- subset(dfMerged, !is.na(Party) & (YOB >= 1930 & YOB <= 2003)) %>% \n            dplyr::select(-USER_ID, -NumAnsS, -YOB, -Q98197)\ndfTest <- subset(dfMerged, is.na(Party))\n```\n\n\n### Base Models\n```{r Base Models}\n# Always Democrat since they have higher proportion in data\nprop.table(table(dfTrain$Party))    # accuracy\n\n```\n\n\n<!-- ### Parameter Tuning by K-fold Cross Validation -->\n\n<!-- ```{r Best Tuning Parameters} -->\n\n<!-- outcome <- \"Party\" -->\n<!-- # all predictors -->\n<!-- vPredictorsAll <- setdiff(names(dfTrain), outcome) -->\n<!-- # subset predictors -->\n<!-- vPredictorsSubset <- c(\"Age_group\", \"Gender\", \"Income\", \"HouseholdStatus\", \"EducationLevel\", \"NumAnsP\", -->\n<!--                        \"Q109244\",\"Q115611\",\"Q113181\",\"Q101163\",\"Q98869\") -->\n\n<!-- getBestTune <- function(modelMethod, outcome, vPredictors, modelTrain, fitControl) { -->\n<!--     y <- modelTrain[,outcome] -->\n<!--     x <- modelTrain[,vPredictors] -->\n<!--     dmy <- dummyVars(~ ., data = x) -->\n<!--     x <- as.data.frame(predict(dmy, x)) -->\n<!--     LINCOMB <- findLinearCombos(x) -->\n<!--     x <- x[, -LINCOMB$remove] -->\n<!--     NZV <- nearZeroVar(x, saveMetrics = TRUE) -->\n<!--     x <- x[, -which(NZV[1:nrow(NZV),]$nzv == TRUE)] -->\n\n<!--     print(paste(\"Tuning parameters for\", modelMethod)) -->\n\n<!--     if(modelMethod == \"gbm\") { -->\n<!--         set.seed(2300) -->\n<!--         model_obj <- train(x, y, method = modelMethod, trControl = fitControl, verbose = F) -->\n<!--     } -->\n<!--     else if(modelMethod == \"rpart\") { -->\n<!--         rpartGrid <- expand.grid(cp = seq(0.001, 0.1, 0.001)) -->\n<!--         model_obj <- train(x, y, method = modelMethod, trControl = fitControl, tuneGrid = rpartGrid) -->\n<!--     } -->\n<!--     else if(modelMethod == \"rf\") { -->\n<!--         rfGrid <- expand.grid(mtry = c(2, 3, 4)) -->\n<!--         set.seed(2300) -->\n<!--         model_obj <- train(x, y, method = modelMethod, importance = F, nodesize = 5,  -->\n<!--                            trControl = fitControl, tuneGrid = rfGrid) -->\n<!--     } -->\n<!--     else { -->\n<!--         set.seed(2300) -->\n<!--         model_obj <- train(x, y, method = modelMethod, trControl = fitControl) -->\n<!--     } -->\n<!--     return(model_obj) -->\n<!-- } -->\n\n<!-- vModelMethods <- c(\"glmnet\", \"gbm\", \"lda\", \"rf\", \"rpart\", \"svmRadial\", \"xgbLinear\") -->\n\n<!-- # Set number of cores for parallel computation in caret -->\n<!-- library(doParallel) -->\n<!-- registerDoParallel(cores = 4) -->\n\n<!-- # Set trainControl for cross-validation -->\n<!-- numfolds <- 5 -->\n<!-- numrepeats <- 5 -->\n<!-- fitControl <- trainControl(method = \"repeatedcv\", number = numfolds, repeats = numrepeats, -->\n<!--                            allowParallel = T, classProbs = T) -->\n\n<!-- lBestTune <- list() -->\n<!-- for(i in 1:length(vModelMethods)) { -->\n<!--     modelMethod <- vModelMethods[i] -->\n<!--     lBestTune[[i]] <- getBestTune(modelMethod, outcome, vPredictorsAll, dfTrain, fitControl) -->\n<!-- } -->\n<!-- names(lBestTune) <- vModelMethods -->\n\n<!-- # Check cross-validation accuracy -->\n<!-- cv.perf <- resamples(lBestTune) -->\n<!-- dotplot(cv.perf, metric = \"Accuracy\") -->\n<!-- summary(cv.perf) -->\n<!-- ``` -->\n\n\n\n<!-- #### Model Probabilities -->\n\n<!-- ```{r Model Probabilities} -->\n<!-- getModelProbabilities <- function(modelMethod, outcome, vPredictors,  -->\n<!--                                   modelTrain, modelTest, dfBestTune = NULL) { -->\n\n<!--     modelFormula <- paste(outcome, paste(vPredictors, collapse = \" + \"), sep = \" ~ \") -->\n<!--     y <- modelTrain[,outcome] -->\n<!--     x <- modelTrain[,vPredictors] -->\n<!--     dmy <- dummyVars(~ ., data = x) -->\n<!--     x <- as.data.frame(predict(dmy, x)) -->\n<!--     LINCOMB <- findLinearCombos(x) -->\n<!--     x <- x[, -LINCOMB$remove] -->\n<!--     NZV <- nearZeroVar(x, saveMetrics = TRUE) -->\n<!--     x <- x[, -which(NZV[1:nrow(NZV),]$nzv == TRUE)] -->\n\n<!--     modelTest <- modelTest[,vPredictors] -->\n<!--     dmy <- dummyVars(~ ., modelTest) -->\n<!--     modelTest <- as.data.frame(predict(dmy, modelTest)) -->\n<!--     modelTest <- modelTest[, -LINCOMB$remove] -->\n<!--     modelTest <- modelTest[, -which(NZV[1:nrow(NZV),]$nzv == TRUE)] -->\n\n<!--     if(modelMethod == \"gbm\") { -->\n<!--         gbmGrid <- expand.grid(n.trees = dfBestTune$n.trees,  -->\n<!--                                interaction.depth = dfBestTune$interaction.depth, -->\n<!--                                shrinkage = dfBestTune$shrinkage, -->\n<!--                                n.minobsinnode = dfBestTune$n.minobsinnode) -->\n<!--         set.seed(2300) -->\n<!--         model_obj <- train(x, y, method = modelMethod, verbose = F, tuneGrid = gbmGrid) -->\n<!--     }  -->\n<!--     else if(modelMethod == \"glm\") { -->\n<!--         model_obj <- train(x, y, method = modelMethod) -->\n<!--     } -->\n<!--     else if(modelMethod == \"glmnet\") { -->\n<!--         glmnetGrid <- expand.grid(alpha = dfBestTune$alpha, lambda = dfBestTune$lambda) -->\n<!--         model_obj <- train(x, y, method = modelMethod, tuneGrid = glmnetGrid) -->\n<!--     } -->\n<!--     else if(modelMethod == \"rf\") { -->\n<!--         set.seed(2300) -->\n<!--         model_obj <- randomForest(x, y, mtry = dfBestTune$mtry, nodesize = 5) -->\n<!--     } -->\n<!--     else if(modelMethod == \"rpart\") { -->\n<!--         rpartGrid <- expand.grid(cp = dfBestTune$cp) -->\n<!--         model_obj <- train(x, y, method = modelMethod, tuneGrid = rpartGrid) -->\n<!--     } -->\n<!--     else if(modelMethod == \"svmRadial\") { -->\n<!--         svmRadialGrid <- expand.grid(sigma = dfBestTune$sigma, C = dfBestTune$C) -->\n<!--         model_obj <- train(x, y, method = modelMethod, tuneGrid = data.frame(sigma = dfBestTune$sigma,C = dfBestTune$C)) -->\n<!--     } -->\n<!--     else if(modelMethod == \"xgbLinear\") { -->\n<!--         xgbGrid <- expand.grid(nrounds = dfBestTune$nrounds, -->\n<!--                                lambda = dfBestTune$lambda, -->\n<!--                                alpha = dfBestTune$alpha, -->\n<!--                                eta = dfBestTune$eta) -->\n<!--         set.seed(2300) -->\n<!--         model_obj <- train(x, y, method = modelMethod, tuneGrid = xgbGrid) -->\n<!--     } -->\n<!--     else { -->\n<!--         set.seed(2300) -->\n<!--         model_obj <- train(x, y, method = modelMethod) -->\n<!--     } -->\n\n<!--     pred_test <- predict(model_obj, modelTest, type = \"prob\") -->\n<!--     return(pred_test[,2]) -->\n<!-- } -->\n<!-- ``` -->\n\n\n<!-- #### GLM Model Median Accuracy -->\n\n<!-- ```{r GLM} -->\n\n<!-- vAccuracy <- c() -->\n<!-- for (i in 1:max(numfolds)) { -->\n<!--     modelTrain <- dfTrain[numfolds != i,] -->\n<!--     modelTest <- dfTrain[numfolds == i,] -->\n\n<!--     # train models and make predictions here -->\n<!--     pred_glm <- getModelProbabilities(modelMethod = \"glm\", outcome, vPredictorsSubset,  -->\n<!--                                       modelTrain, modelTest) -->\n<!--     dfAccuracy <- getModelCharacteristics(modelTest$Party, pred_glm) -->\n<!--     vAccuracy <- c(vAccuracy, as.numeric(dfAccuracy$accuracy)) -->\n<!-- } -->\n<!-- medianAccuracy <- format(median(vAccuracy), digits = 5) -->\n<!-- print(paste(\"Median glm accuracy =\", medianAccuracy)) -->\n<!-- ``` -->\n\n\n<!-- ### Choose Final Model for Kaggle submission -->\n\n<!-- ```{r FinalModel} -->\n\n<!-- vModelMethods <- c(\"gbm\", \"glmnet\", \"rpart\", \"rf\", \"xgbLinear\") -->\n<!-- lModelProb <- list() -->\n<!-- for(i in 1:length(vModelMethods)) { -->\n<!--     modelMethod <- vModelMethods[i] -->\n<!--     print(paste(\"Getting test set probabilities for\", modelMethod)) -->\n<!--     lModelProb[[i]] <- getModelProbabilities(modelMethod, outcome, vPredictorsAll,  -->\n<!--                                              dfTrain, dfTest, lBestTune[[modelMethod]]$bestTune) -->\n<!-- } -->\n<!-- names(lModelProb) <- vModelMethods -->\n<!-- dfFinalProb <- as.data.frame(lModelProb) -->\n\n<!-- dfFinalProb$MeanModel <- apply(dfFinalProb, 1, function(Z) mean(as.numeric(Z[1:length(vModelMethods)]), na.rm = T)) -->\n<!-- dfFinalProb$MedianModel <- apply(dfFinalProb, 1, function(Z) median(as.numeric(Z[1:length(vModelMethods)]), na.rm = T)) -->\n\n<!-- finalModel <- \"MeanModel\" -->\n<!-- PredTest <- dfFinalProb[finalModel] -->\n\n<!-- threshold = 0.5 -->\n<!-- vPredTestLabels = as.factor(ifelse(PredTest<threshold, \"Democrat\", \"Republican\")) -->\n\n<!-- dfSubmit = data.frame(USER_ID = dfTest$USER_ID, PREDICTIONS = vPredTestLabels) -->\n<!-- write.csv(dfSubmit, paste0(\"submission_\", finalModel, \".csv\"), quote = F, row.names=FALSE) -->\n<!-- ``` -->\n\n",
    "created" : 1465846656446.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1526942253",
    "id" : "564E0D00",
    "lastKnownWriteTime" : 1482778168,
    "last_content_update" : 1482778168149,
    "path" : "C:/Backups/Kaggle/VotersParty/VotersParty.Rmd",
    "project_path" : "VotersParty.Rmd",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}